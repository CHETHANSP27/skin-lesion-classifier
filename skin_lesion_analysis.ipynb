{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b101c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376da5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed: 1600 train, 400 val samples\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "df = pd.read_csv(\"sample_metadata.csv\")\n",
    "\n",
    "# Rename columns for clarity (optional)\n",
    "df.rename(columns={\n",
    "    'dx': 'diagnosis',\n",
    "    'localization': 'anatom_site_general_challenge'\n",
    "}, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "df['sex'] = df['sex'].fillna('unknown')\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].fillna('unknown')\n",
    "\n",
    "# Encode categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoders = {}\n",
    "for col in ['sex', 'anatom_site_general_challenge', 'diagnosis']:\n",
    "    enc = LabelEncoder()\n",
    "    df[col] = enc.fit_transform(df[col])\n",
    "    encoders[col] = enc\n",
    "\n",
    "# Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['diagnosis'], random_state=42)\n",
    "\n",
    "print(f\"âœ… Preprocessed: {len(train_df)} train, {len(val_df)} val samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ded2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, num_tabular_features=3, num_classes=7):\n",
    "        super(HybridNet, self).__init__()\n",
    "        \n",
    "        # Image branch (ResNet18 backbone)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()  # Remove final layer (we'll fuse manually)\n",
    "        image_feat_dim = 512  # ResNet18 final feature size\n",
    "        \n",
    "        # Tabular branch\n",
    "        self.tabular_net = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Fusion + Final Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_feat_dim + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, tabular):\n",
    "        image_features = self.cnn(image)            # (batch_size, 512)\n",
    "        tabular_features = self.tabular_net(tabular)  # (batch_size, 32)\n",
    "        combined = torch.cat([image_features, tabular_features], dim=1)  # (batch_size, 544)\n",
    "        out = self.classifier(combined)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafeae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cheth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cheth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\cheth/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:04<00:00, 10.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridNet(num_tabular_features=3, num_classes=len(encoders['diagnosis'].classes_)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022932a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    return (predicted == labels).sum().item() / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = 0, 0\n",
    "        model.train()\n",
    "        for images, tabular, labels in train_loader:\n",
    "            images, tabular, labels = images.to(device), tabular.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, tabular)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = accuracy(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc\n",
    "\n",
    "        val_loss, val_acc = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, tabular, labels in val_loader:\n",
    "                images, tabular, labels = images.to(device), tabular.to(device), labels.to(device)\n",
    "                outputs = model(images, tabular)\n",
    "                loss = criterion(outputs, labels)\n",
    "                acc = accuracy(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc\n",
    "\n",
    "        # Logging\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_acc'].append(train_acc / len(train_loader))\n",
    "        history['val_loss'].append(val_loss / len(val_loader))\n",
    "        history['val_acc'].append(val_acc / len(val_loader))\n",
    "\n",
    "        print(f\"ðŸ“˜ Epoch {epoch+1}/{epochs} | Train Acc: {train_acc/len(train_loader):.4f} | Val Acc: {val_acc/len(val_loader):.4f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba3c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row['image_id'] + \".jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        tabular = torch.tensor([\n",
    "            row['age'],\n",
    "            row['sex'],\n",
    "            row['anatom_site_general_challenge']\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n",
    "        return image, tabular, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133dbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset from earlier class\n",
    "train_dataset = SkinLesionDataset(train_df, \"images\")\n",
    "val_dataset = SkinLesionDataset(val_df, \"images\")\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adeada1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Epoch 1/5 | Train Loss: 1.6050 | Train Acc: 0.5075 | Val Acc: 0.7308\n",
      "ðŸ“˜ Epoch 2/5 | Train Loss: 1.0218 | Train Acc: 0.7644 | Val Acc: 0.7788\n",
      "ðŸ“˜ Epoch 3/5 | Train Loss: 0.6285 | Train Acc: 0.8888 | Val Acc: 0.7788\n",
      "ðŸ“˜ Epoch 4/5 | Train Loss: 0.4385 | Train Acc: 0.9369 | Val Acc: 0.7404\n",
      "ðŸ“˜ Epoch 5/5 | Train Loss: 0.2997 | Train Acc: 0.9688 | Val Acc: 0.7788\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b5461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Epoch 1/5 | Train Loss: 0.1919 | Train Acc: 0.9869 | Val Acc: 0.7788\n",
      "ðŸ“˜ Epoch 2/5 | Train Loss: 0.1279 | Train Acc: 0.9938 | Val Acc: 0.7716\n",
      "ðŸ“˜ Epoch 3/5 | Train Loss: 0.1011 | Train Acc: 0.9975 | Val Acc: 0.7764\n",
      "ðŸ“˜ Epoch 4/5 | Train Loss: 0.0805 | Train Acc: 0.9988 | Val Acc: 0.7981\n",
      "ðŸ“˜ Epoch 5/5 | Train Loss: 0.0701 | Train Acc: 0.9988 | Val Acc: 0.7837\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      9\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m plt.plot(\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_acc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label=\u001b[33m'\u001b[39m\u001b[33mTrain Acc\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m plt.plot(history[\u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mVal Acc\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mAccuracy over Epochs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run training\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91293df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
